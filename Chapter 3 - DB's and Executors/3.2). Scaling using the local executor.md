# Scaling.

As with the previous exercise, there are cases where we want to be running tasks in parallel, rather than one after another.

Before we can do this, we need to do two things:

* Change the database, the default SQLite database only allows for sequential running of tasks (doesn't allow for multiple writes), hence this will not work. In our example, we will use PostgreSQL.
* We need to change the executor from the sequential executor to the LOCAL executor, this allows us to run multiple tasks at the same time on the same machine.

Let's install Postgres on our machine:

1). ```sudo apt update``` - updates existing packages.

2). ```sudo apt install postgresql``` - installs PostgreSQL.

3). ```sudo -u postgres psql``` - connect to Postgres.

4). ```ALTER USER postgres PASSWORD '<password>';``` - specify password for user

Let's install Postgres package in our python virtualenv. & connect it with Airflow:

(Inside sandbox virtualenv.).

1). ```pip install 'apache-airflow[postgres]'```

2). Open up airflow.cfg in your text editor and search for 'sql_alchemy_conn'.

3). Replace the sqlite string with the postgres db string: ```postgresql+psycopg2://<username>:<password>@localhost/postgres```

4). ```airflow db check``` - checks the connection, returns success if configured correctly.

Let's change the executor:

1). In the airflow.cfg file, look for 'executor'.

2). Replace ```SequentialExecutor``` with ```LocalExecutor```.

3). Close Airflow (close webserver & scheduler).

**Now, we can initialise the Postgres DB:**

1). ```airflow db init``` - re-initialises Airflow with the default Postgres DB.

2). Create a new user using the following CLI commands as before:\
```airflow users create -u <username> -p <password> -f <first_name> -l <last_name> -r <role_privileges> -e <email_address>```

3). Relaunch the webserver & scheduler and re-run the DAG.

The GANTT view should be a good indication of whether the Tasks are now running in parallel:

![](../images/Parallel%20GANTT.png)