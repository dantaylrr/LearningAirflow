# Airflow's default config.

How can schedule or trigger multiple tasks to run side by side? \
How can we scale Airflow to run as many tasks as we want?

### What is Airflow's default config?

Let's use an example:\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -----> Task 2

Task 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -----> Task 4

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -----> Task 3

Here, we have both Task 2 & Task 3 depending on Task 1's completion, Airflow's default config will not specify in which order these two tasks will be run.

We can alter Airflow's config to outline which task we want to complete in what order when two tasks have a common dependency.

The default executor in Airflow is the ***sequential*** executor, meaning that tasks are completed in a specified sequence. Thus, using the example above, Tasks 2 & 3 will be run one after another, rather than at parallel.

**Some useful CLI syntax:**

```airflow config get-value core sql_alchemy_conn``` - will get the default Airflow db connection, by default this is SQLite unless specified otherwise.

```airflow config get-value core executor``` - will give us the default executor.

### Exercise.

Create a simple DAG that replicates the structure above, using simple bash operators with the 'sleep 3' command.