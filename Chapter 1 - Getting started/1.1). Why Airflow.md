# Why Airflow?

### Use Case.

We have a data pipeline that we want to trigger every day at 10pm, it looks a little like the following:

Download the data we want to use. **----->** Process the data. **----->** Store the data.

These jobs may interact with other tools or pieces of software during this process. For example:

* We may get the data using an API.
* We may use Apache Spark to process the data if it's a substantial job.
* We may have to make insertions/updates/deletions to a SQL server.

Now:

* What happens if the API isn't available? 
* What happens if the Spark job fails? 
* What happens if we cannot update the SQL server?

Furthermore:

**What happens if you have hundreds of data pipelines?**

**Solution**: *Apache Airflow*

With Airflow we can manage our data pipelines & execute tasks in a reliable way, as well as being able to monitor them automatically.